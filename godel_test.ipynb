{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ann-SophieBlank/.pyenv/versions/MHConvoAI/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/GODEL-v1_1-base-seq2seq\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"microsoft/GODEL-v1_1-base-seq2seq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input, history=[]):\n",
    "\n",
    "    instruction = 'Instruction: given a dialog context, you need to response empathically'\n",
    "\n",
    "    knowledge = '  '\n",
    "\n",
    "    s = list(sum(history, ()))\n",
    "\n",
    "    s.append(input)\n",
    "\n",
    "    #print(s)\n",
    "\n",
    "    dialog = ' EOS ' .join(s)\n",
    "\n",
    "    #print(dialog)\n",
    "\n",
    "    query = f\"{instruction} [CONTEXT] {dialog} {knowledge}\"\n",
    "\n",
    "    top_p = 0.9\n",
    "    min_length = 8\n",
    "    max_length = 200\n",
    "\n",
    "\n",
    "    # tokenize the new input sentence\n",
    "    new_user_input_ids = tokenizer.encode(f\"{query}\", return_tensors='pt')\n",
    "\n",
    "\n",
    "    output = model.generate(new_user_input_ids, \n",
    "                            min_length=int(min_length), \n",
    "                            max_length=int(max_length), \n",
    "                            top_p=top_p, \n",
    "                            do_sample=True).tolist()\n",
    "    \n",
    "  \n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    " \n",
    "    history.append((input, response))\n",
    "    \n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"My girlfriend doesn't want to move in with me. I'm lonely and want to get married and have 17 children.\",\n",
       "  'I’m assuming you have some sort of plan or plan for a wedding, but I also have a plan in mind that you’d like to have a wedding and you have children, so you’re not alone.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input2 = \"My girlfriend doesn't want to move in with me. I'm lonely and want to get married and have 17 children.\"\n",
    "predict(input2, history=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#knowledge = fine tuned mental health data\n",
    "def generate(knowledge, input, history=[]):\n",
    "    #instruction for model on how to structure response\n",
    "    instruction = 'Instruction: given a dialog context, you need to response empathically'\n",
    "    #dialog history is a copy of the history variable + new input\n",
    "    s = list(sum(history, ()))\n",
    "    s.append(input)\n",
    "    #dialog_history = list(history).append(input)\n",
    "\n",
    "#    if knowledge != '':\n",
    " #       knowledge = '[KNOWLEDGE] ' + knowledge\n",
    "\n",
    "    dialog = ' EOS '.join(s)\n",
    "\n",
    "    query = f\"{instruction} [CONTEXT] {dialog} {knowledge}\"\n",
    "\n",
    "    top_p = 0.9\n",
    "    min_length = 8\n",
    "    max_length = 200\n",
    "\n",
    "    new_user_input_ids = tokenizer.encode(f\" (query)\", return_tensors='pt')\n",
    "    outputs = model.generate(new_user_input_ids,\n",
    "                             max_length=int(max_length),\n",
    "                             min_length=int(min_length),\n",
    "                             top_p=top_p,\n",
    "                             do_sample=True).tolist()\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge1 = ''\n",
    "history1 = ['I want to kill myself', 'Suicide is not an option.']\n",
    "input1 = \"My relationship feels off and I feel insecure\tMy girlfriend's grandma passed away 5 months ago. They were very close. She took care of her till she died. Things kinda returned to normal few weeks later. Last month it feels like we hit a brick wall. Intimacy fell off. I asked what's up. She says she can't connect with anyone and that it's not me. She used to be very open and expressive. Now she gives short answers and has no interest in sex or any touching. When we did have sex in the last month, something felt really off. Now I'm very insecure about us and have thoughts of her cheating. She says otherwise, but I don't know. It just feels like something is really off.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate tuple (not \"str\") to tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m generate(knowledge1, history1, input1)\n",
      "Cell \u001b[0;32mIn[78], line 6\u001b[0m, in \u001b[0;36mgenerate\u001b[0;34m(knowledge, input, history)\u001b[0m\n\u001b[1;32m      4\u001b[0m     instruction \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mInstruction: given a dialog context, you need to response empathically\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      5\u001b[0m     \u001b[39m#dialog history is a copy of the history variable + new input\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     s \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39msum\u001b[39;49m(history, ()))\n\u001b[1;32m      7\u001b[0m     s\u001b[39m.\u001b[39mappend(\u001b[39minput\u001b[39m)\n\u001b[1;32m      8\u001b[0m     \u001b[39m#dialog_history = list(history).append(input)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[39m#    if knowledge != '':\u001b[39;00m\n\u001b[1;32m     11\u001b[0m  \u001b[39m#       knowledge = '[KNOWLEDGE] ' + knowledge\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate tuple (not \"str\") to tuple"
     ]
    }
   ],
   "source": [
    "generate(knowledge1, history1, input1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit ('shims')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1875e3b4d47a39b140ad93b3e0c174c085ac6dc8de52e317e3cc5345dcbc697b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
