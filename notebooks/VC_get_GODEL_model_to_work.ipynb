{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "! pip install transformers gradio -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wIM_bRRUBhn",
        "outputId": "755d338a-e223-4c3c-8821-d8b21647681f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 11.6 MB 4.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 15.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 84 kB 4.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 278 kB 67.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 106 kB 71.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 54 kB 3.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 55 kB 4.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 84 kB 3.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 213 kB 67.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 64 kB 3.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 80 kB 10.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 68 kB 7.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 68 kB 7.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 68 kB 7.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 68 kB 7.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 50 kB 7.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 64.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 856 kB 26.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 593 kB 26.1 MB/s \n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for python-multipart (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "PT_xvWA1UF--"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/GODEL-v1_1-base-seq2seq\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"microsoft/GODEL-v1_1-base-seq2seq\")"
      ],
      "metadata": {
        "id": "v9n7PtaHUGMs"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(input, history=[]):\n",
        "\n",
        "    instruction = 'Instruction: given a dialog context, you need to response empathically'\n",
        "\n",
        "    knowledge = '  '\n",
        "\n",
        "    s = list(sum(history, ()))\n",
        "\n",
        "    s.append(input)\n",
        "\n",
        "    #print(s)\n",
        "\n",
        "    dialog = ' EOS ' .join(s)\n",
        "\n",
        "    #print(dialog)\n",
        "\n",
        "    query = f\"{instruction} [CONTEXT] {dialog} {knowledge}\"\n",
        "\n",
        "    top_p = 0.9\n",
        "    min_length = 8\n",
        "    max_length = 200\n",
        "\n",
        "\n",
        "    # tokenize the new input sentence\n",
        "    new_user_input_ids = tokenizer.encode(f\"{query}\", return_tensors='pt')\n",
        "\n",
        "\n",
        "    output = model.generate(new_user_input_ids, \n",
        "                            min_length=int(min_length), \n",
        "                            max_length=int(max_length), \n",
        "                            top_p=top_p, \n",
        "                            do_sample=True).tolist()\n",
        "    \n",
        "  \n",
        "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        " \n",
        "    history.append((input, response))\n",
        "    \n",
        "    return history"
      ],
      "metadata": {
        "id": "wOccIYoAUGQf"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict2(input, history=[]):\n",
        "\n",
        "    # Basic instruction\n",
        "    instruction = 'Instruction: given a dialog context, you need to response empathically.'\n",
        "\n",
        "    # Knowledge given\n",
        "    knowledge = '  '\n",
        "\n",
        "    # Copy history of dialog\n",
        "    dialog_list = history.copy()\n",
        "    print('history of dialog: ', dialog_list)\n",
        "\n",
        "    # Append user input to dialog\n",
        "    dialog_list.append(input)\n",
        "    print('new input appended: ', dialog_list)\n",
        "\n",
        "    # Prepare dialog to be used in the model\n",
        "    dialog = ' EOS ' .join(dialog_list)\n",
        "    print('dialog prepared for the model: ', dialog)\n",
        "\n",
        "    # Build query\n",
        "    query = f\"{instruction} [CONTEXT] {dialog} {knowledge}\"\n",
        "    print('query: ', query)\n",
        "\n",
        "    # Tokenize the new input sentence\n",
        "    new_user_input_ids = tokenizer.encode(f\"{query}\", return_tensors='pt')\n",
        "    print('Output of the tokenizer: ', new_user_input_ids)\n",
        "\n",
        "    # Set params for model output\n",
        "    top_p = 0.9 # What is this???\n",
        "    min_length = 8\n",
        "    max_length = 200\n",
        "\n",
        "    # Generate output\n",
        "    output = model.generate(new_user_input_ids, \n",
        "                            min_length=int(min_length), \n",
        "                            max_length=int(max_length), \n",
        "                            top_p=top_p, \n",
        "                            do_sample=True).tolist()\n",
        "    print('Output of the model: ', output)          \n",
        "\n",
        "    # Decode output\n",
        "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    print('Decoded output: ', response)     \n",
        "\n",
        "    # Create new history of dialog\n",
        "    dialog_list.append(response)\n",
        "\n",
        "    return response, dialog_list"
      ],
      "metadata": {
        "id": "T-FRL3mKTmWm"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First user input\n",
        "input1 = 'I am sad.'"
      ],
      "metadata": {
        "id": "t-_2rdLjUH4y"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First response\n",
        "response1, history1 = predict2(input1)\n",
        "print(response1)\n",
        "print(history1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJaxlNhLh0Pe",
        "outputId": "1c489fc7-1dd2-4962-b49a-1161d59a5642"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "history of dialog:  []\n",
            "new input appended:  ['I am sad.']\n",
            "dialog prepared for the model:  I am sad.\n",
            "query:  Instruction: given a dialog context, you need to response empathically. [CONTEXT] I am sad.   \n",
            "Output of the tokenizer:  tensor([[21035,    10,   787,     3,     9, 13463,  2625,     6,    25,   174,\n",
            "            12,  1773,  8943,     9,   189,  6402,     5,   784, 17752,  3463,\n",
            "             4,   382,   908,    27,   183,  6819,     5,     1]])\n",
            "Output of the model:  [[0, 27, 183, 78, 8032, 24, 25, 33, 352, 190, 48, 5, 1]]\n",
            "Decoded output:  I am so sorry that you are going through this.\n",
            "I am so sorry that you are going through this.\n",
            "['I am sad.', 'I am so sorry that you are going through this.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Second user input\n",
        "input2 = 'Can you help me?'\n"
      ],
      "metadata": {
        "id": "gd8UDajSKp_A"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Second response\n",
        "response2, history2 = predict2(input2, history1)\n",
        "print(response2)\n",
        "print(history2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1haMuDfKqDA",
        "outputId": "a82e167b-edcd-4bbb-f2db-d816153774e0"
      },
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "history of dialog:  ['I am sad.', 'I am so sorry that you are going through this.']\n",
            "new input appended:  ['I am sad.', 'I am so sorry that you are going through this.', 'Can you help me?']\n",
            "dialog prepared for the model:  I am sad. EOS I am so sorry that you are going through this. EOS Can you help me?\n",
            "query:  Instruction: given a dialog context, you need to response empathically. [CONTEXT] I am sad. EOS I am so sorry that you are going through this. EOS Can you help me?   \n",
            "Output of the tokenizer:  tensor([[21035,    10,   787,     3,     9, 13463,  2625,     6,    25,   174,\n",
            "            12,  1773,  8943,     9,   189,  6402,     5,   784, 17752,  3463,\n",
            "             4,   382,   908,    27,   183,  6819,     5, 32100,    27,   183,\n",
            "            78,  8032,    24,    25,    33,   352,   190,    48,     5, 32100,\n",
            "          1072,    25,   199,   140,    58,     1]])\n",
            "Output of the model:  [[0, 863, 752, 140, 214, 24, 27, 183, 352, 190, 48, 5, 1]]\n",
            "Decoded output:  Please let me know that I am going through this.\n",
            "Please let me know that I am going through this.\n",
            "['I am sad.', 'I am so sorry that you are going through this.', 'Can you help me?', 'Please let me know that I am going through this.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Third user input\n",
        "input3 = 'I do not manage to deal with it alone...'"
      ],
      "metadata": {
        "id": "1ZXZrLXvUPFU"
      },
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Third response\n",
        "response3, history3 = predict2(input3, history2)\n",
        "print(response3)\n",
        "print(history3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfInPYoVUPIe",
        "outputId": "981dbab1-3cbe-4035-915f-362e283ce18c"
      },
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "history of dialog:  ['I am sad.', 'I am so sorry that you are going through this.', 'Can you help me?', 'Please let me know that I am going through this.']\n",
            "new input appended:  ['I am sad.', 'I am so sorry that you are going through this.', 'Can you help me?', 'Please let me know that I am going through this.', 'I do not manage to deal with it alone...']\n",
            "dialog prepared for the model:  I am sad. EOS I am so sorry that you are going through this. EOS Can you help me? EOS Please let me know that I am going through this. EOS I do not manage to deal with it alone...\n",
            "query:  Instruction: given a dialog context, you need to response empathically. [CONTEXT] I am sad. EOS I am so sorry that you are going through this. EOS Can you help me? EOS Please let me know that I am going through this. EOS I do not manage to deal with it alone...   \n",
            "Output of the tokenizer:  tensor([[21035,    10,   787,     3,     9, 13463,  2625,     6,    25,   174,\n",
            "            12,  1773,  8943,     9,   189,  6402,     5,   784, 17752,  3463,\n",
            "             4,   382,   908,    27,   183,  6819,     5, 32100,    27,   183,\n",
            "            78,  8032,    24,    25,    33,   352,   190,    48,     5, 32100,\n",
            "          1072,    25,   199,   140,    58, 32100,   863,   752,   140,   214,\n",
            "            24,    27,   183,   352,   190,    48,     5, 32100,    27,   103,\n",
            "            59,  1865,    12,  1154,    28,    34,  2238,   233,     1]])\n",
            "Output of the model:  [[0, 27, 214, 24, 5, 27, 473, 8032, 21, 25, 5, 1]]\n",
            "Decoded output:  I know that. I feel sorry for you.\n",
            "I know that. I feel sorry for you.\n",
            "['I am sad.', 'I am so sorry that you are going through this.', 'Can you help me?', 'Please let me know that I am going through this.', 'I do not manage to deal with it alone...', 'I know that. I feel sorry for you.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DfO_qkP4LcKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pHR6krzTUPPI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RQRQWIjWUPSh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jxj3XsLFUPWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gBTL947pUGU_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rQBJIn-sUGXm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bH-QOH0RUGa-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}