{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ann-SophieBlank/.pyenv/versions/MHConvoAI/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/GODEL-v1_1-base-seq2seq\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"microsoft/GODEL-v1_1-base-seq2seq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input, history=[]):\n",
    "\n",
    "    instruction = 'Instruction: given a dialog context, you need to response empathically'\n",
    "\n",
    "    knowledge = '  '\n",
    "\n",
    "    s = list(sum(history, ()))\n",
    "\n",
    "    s.append(input)\n",
    "\n",
    "    #print(s)\n",
    "\n",
    "    dialog = ' EOS ' .join(s)\n",
    "\n",
    "    #print(dialog)\n",
    "\n",
    "    query = f\"{instruction} [CONTEXT] {dialog} {knowledge}\"\n",
    "\n",
    "    top_p = 0.8\n",
    "    min_length = 8\n",
    "    max_length = 200\n",
    "\n",
    "\n",
    "    # tokenize the new input sentence\n",
    "    new_user_input_ids = tokenizer.encode(f\"{query}\", return_tensors='pt')\n",
    "\n",
    "\n",
    "    output = model.generate(new_user_input_ids, \n",
    "                            min_length=int(min_length), \n",
    "                            max_length=int(max_length), \n",
    "                            top_p=top_p, \n",
    "                            do_sample=True).tolist()\n",
    "    \n",
    "  \n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    " \n",
    "    history.append((input, response))\n",
    "    \n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"My girlfriend doesn't want to move in with me. I'm lonely and want to get married and have 17 children.\",\n",
       "  'Well you need to get out of the relationship.')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input2 = \"My girlfriend doesn't want to move in with me. I'm lonely and want to get married and have 17 children.\"\n",
    "predict(input2, history=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#knowledge = fine tuned mental health data\n",
    "def generate(knowledge, input, history=[]):\n",
    "    #instruction for model on how to structure response\n",
    "    instruction = 'Instruction: given a dialog context, you need to response empathically'\n",
    "    #dialog history is a copy of the history variable + new input\n",
    "    s = history.copy()\n",
    "    s.append(input)\n",
    "    #dialog_history = list(history).append(input)\n",
    "\n",
    "#    if knowledge != '':\n",
    " #       knowledge = '[KNOWLEDGE] ' + knowledge\n",
    "\n",
    "    dialog = ' EOS '.join(s)\n",
    "\n",
    "    query = f\"{instruction} [CONTEXT] {dialog} {knowledge}\"\n",
    "\n",
    "    top_p = 0.9\n",
    "    min_length = 8\n",
    "    max_length = 200\n",
    "\n",
    "    new_user_input_ids = tokenizer.encode(f\" (query)\", return_tensors='pt')\n",
    "    outputs = model.generate(new_user_input_ids,\n",
    "                             max_length=int(max_length),\n",
    "                             min_length=int(min_length),\n",
    "                             top_p=top_p,\n",
    "                             do_sample=True).tolist()\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge1 = ''\n",
    "history1 = ['I want to kill myself.', 'Suicide is not an option.']\n",
    "input1 = \"My relationship feels off and I feel insecure\tMy girlfriend's grandma passed away 5 months ago. They were very close. She took care of her till she died. Things kinda returned to normal few weeks later. Last month it feels like we hit a brick wall. Intimacy fell off. I asked what's up. She says she can't connect with anyone and that it's not me. She used to be very open and expressive. Now she gives short answers and has no interest in sex or any touching. When we did have sex in the last month, something felt really off. Now I'm very insecure about us and have thoughts of her cheating. She says otherwise, but I don't know. It just feels like something is really off.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It's really strange. It just makes you feel a little sad\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(knowledge1, input1, history=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit ('shims')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Oct 18 2022, 11:40:18) [Clang 14.0.0 (clang-1400.0.29.102)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1875e3b4d47a39b140ad93b3e0c174c085ac6dc8de52e317e3cc5345dcbc697b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
