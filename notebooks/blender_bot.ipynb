{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":3583,"status":"ok","timestamp":1670430457083,"user":{"displayName":"Alissa Huynh","userId":"05064198293916290951"},"user_tz":-240},"id":"kOIeKGEwQejh"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33m  DEPRECATION: ffmpy is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33m  DEPRECATION: python-multipart is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33m  DEPRECATION: pycryptodome is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["! pip install transformers gradio -q"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1670430457084,"user":{"displayName":"Alissa Huynh","userId":"05064198293916290951"},"user_tz":-240},"id":"LRS9vYGPQsU_"},"outputs":[],"source":["from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration\n","import torch\n","import gradio as gr"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":3457,"status":"ok","timestamp":1670430460535,"user":{"displayName":"Alissa Huynh","userId":"05064198293916290951"},"user_tz":-240},"id":"75-rTGUBQ2qO"},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading: 100%|██████████| 127k/127k [00:00<00:00, 407kB/s] \n","Downloading: 100%|██████████| 62.9k/62.9k [00:00<00:00, 553kB/s]\n","Downloading: 100%|██████████| 1.15k/1.15k [00:00<00:00, 342kB/s]\n","Downloading: 100%|██████████| 16.0/16.0 [00:00<00:00, 5.72kB/s]\n","Downloading: 100%|██████████| 772/772 [00:00<00:00, 178kB/s]\n","Downloading: 100%|██████████| 1.57k/1.57k [00:00<00:00, 555kB/s]\n","Downloading: 100%|██████████| 730M/730M [00:35<00:00, 20.3MB/s] \n"]}],"source":["tokenizer = BlenderbotTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n","model = BlenderbotForConditionalGeneration.from_pretrained(\"facebook/blenderbot-400M-distill\")"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1670430460536,"user":{"displayName":"Alissa Huynh","userId":"05064198293916290951"},"user_tz":-240},"id":"hPinUeeiQ3cz"},"outputs":[],"source":["def predict(input, history=[]):\n","\n","    if len(history) != 0:\n","      input = ' '.join((*history, input))\n","\n","    inputs_token = tokenizer(input, return_tensors='pt')\n","    # print(inputs_token)\n","\n","    res = model.generate(**inputs_token)\n","    outputs = tokenizer.decode(res[0])\n","\n","    history.append(' '.join((input, outputs)))\n","\n","    return outputs, history"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1670430460536,"user":{"displayName":"Alissa Huynh","userId":"05064198293916290951"},"user_tz":-240},"id":"RNjtbX9sRcJ-"},"outputs":[],"source":["input1 = \"I'm so lonely.\""]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20861,"status":"ok","timestamp":1670430481937,"user":{"displayName":"Alissa Huynh","userId":"05064198293916290951"},"user_tz":-240},"id":"z4QF_s5hRgsZ","outputId":"7156b724-f9aa-42dd-f367-6927f375c1e5"},"outputs":[{"ename":"IndexError","evalue":"index out of range in self","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m response1, history1 \u001b[39m=\u001b[39m predict(input1)\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(response1)\n","Cell \u001b[0;32mIn[5], line 9\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(input, history)\u001b[0m\n\u001b[1;32m      6\u001b[0m inputs_token \u001b[39m=\u001b[39m tokenizer(\u001b[39minput\u001b[39m, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[39m# print(inputs_token)\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m res \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs_token)\n\u001b[1;32m     10\u001b[0m outputs \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(res[\u001b[39m0\u001b[39m])\n\u001b[1;32m     12\u001b[0m history\u001b[39m.\u001b[39mappend(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin((\u001b[39minput\u001b[39m, outputs)))\n","File \u001b[0;32m~/.pyenv/versions/MHConvoAI/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[0;32m~/.pyenv/versions/MHConvoAI/lib/python3.10/site-packages/transformers/generation/utils.py:1367\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, penalty_alpha, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, suppress_tokens, begin_suppress_tokens, forced_decoder_ids, **model_kwargs)\u001b[0m\n\u001b[1;32m   1359\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m   1360\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mA decoder-only architecture is being used, but right-padding was detected! For correct \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1361\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mgeneration results, please set `padding_side=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m\u001b[39m` when initializing the tokenizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1362\u001b[0m         )\n\u001b[1;32m   1364\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mencoder_outputs\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m model_kwargs:\n\u001b[1;32m   1365\u001b[0m     \u001b[39m# if model is encoder decoder encoder_outputs are created\u001b[39;00m\n\u001b[1;32m   1366\u001b[0m     \u001b[39m# and added to `model_kwargs`\u001b[39;00m\n\u001b[0;32m-> 1367\u001b[0m     model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_encoder_decoder_kwargs_for_generation(\n\u001b[1;32m   1368\u001b[0m         inputs_tensor, model_kwargs, model_input_name\n\u001b[1;32m   1369\u001b[0m     )\n\u001b[1;32m   1371\u001b[0m \u001b[39m# 4. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[1;32m   1372\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder:\n","File \u001b[0;32m~/.pyenv/versions/MHConvoAI/lib/python3.10/site-packages/transformers/generation/utils.py:601\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name)\u001b[0m\n\u001b[1;32m    599\u001b[0m encoder_kwargs[\u001b[39m\"\u001b[39m\u001b[39mreturn_dict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    600\u001b[0m encoder_kwargs[model_input_name] \u001b[39m=\u001b[39m inputs_tensor\n\u001b[0;32m--> 601\u001b[0m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39mencoder_outputs\u001b[39m\u001b[39m\"\u001b[39m]: ModelOutput \u001b[39m=\u001b[39m encoder(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mencoder_kwargs)\n\u001b[1;32m    603\u001b[0m \u001b[39mreturn\u001b[39;00m model_kwargs\n","File \u001b[0;32m~/.pyenv/versions/MHConvoAI/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/.pyenv/versions/MHConvoAI/lib/python3.10/site-packages/transformers/models/blenderbot/modeling_blenderbot.py:745\u001b[0m, in \u001b[0;36mBlenderbotEncoder.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[39mif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    743\u001b[0m     inputs_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_tokens(input_ids) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_scale\n\u001b[0;32m--> 745\u001b[0m embed_pos \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_positions(input_shape)\n\u001b[1;32m    747\u001b[0m hidden_states \u001b[39m=\u001b[39m inputs_embeds \u001b[39m+\u001b[39m embed_pos\n\u001b[1;32m    748\u001b[0m hidden_states \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mdropout(hidden_states, p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n","File \u001b[0;32m~/.pyenv/versions/MHConvoAI/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[0;32m~/.pyenv/versions/MHConvoAI/lib/python3.10/site-packages/transformers/models/blenderbot/modeling_blenderbot.py:125\u001b[0m, in \u001b[0;36mBlenderbotLearnedPositionalEmbedding.forward\u001b[0;34m(self, input_ids_shape, past_key_values_length)\u001b[0m\n\u001b[1;32m    121\u001b[0m bsz, seq_len \u001b[39m=\u001b[39m input_ids_shape[:\u001b[39m2\u001b[39m]\n\u001b[1;32m    122\u001b[0m positions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(\n\u001b[1;32m    123\u001b[0m     past_key_values_length, past_key_values_length \u001b[39m+\u001b[39m seq_len, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdevice\n\u001b[1;32m    124\u001b[0m )\n\u001b[0;32m--> 125\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mforward(positions)\n","File \u001b[0;32m~/.pyenv/versions/MHConvoAI/lib/python3.10/site-packages/torch/nn/modules/sparse.py:160\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 160\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    161\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    162\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n","File \u001b[0;32m~/.pyenv/versions/MHConvoAI/lib/python3.10/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n","\u001b[0;31mIndexError\u001b[0m: index out of range in self"]}],"source":["response1, history1 = predict(input1)\n","print(response1)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1670430481938,"user":{"displayName":"Alissa Huynh","userId":"05064198293916290951"},"user_tz":-240},"id":"mo3LH9gmWd7g","outputId":"99899bce-cb95-4069-d24e-b2a64f2182dc"},"outputs":[{"name":"stdout","output_type":"stream","text":["[\"Hi. I feel sad and I would love to get some help. <s> I'm sorry to hear that. I hope you feel better soon. What's going on?</s>\"]\n"]}],"source":["print(history1)"]},{"cell_type":"code","execution_count":104,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1670430481939,"user":{"displayName":"Alissa Huynh","userId":"05064198293916290951"},"user_tz":-240},"id":"R65yVO7DRiJP"},"outputs":[],"source":["input2 = 'Can you help me feeling better?'\n"]},{"cell_type":"code","execution_count":105,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22024,"status":"ok","timestamp":1670430503955,"user":{"displayName":"Alissa Huynh","userId":"05064198293916290951"},"user_tz":-240},"id":"8asgkrzLWXTN","outputId":"a27bbea1-5f28-46ba-c2b8-edf740a58025"},"outputs":[{"name":"stdout","output_type":"stream","text":["<s> I wish I could, but I can't afford to go to the doctor right now. </s>\n"]}],"source":["response2, history2 = predict(input2, history1)\n","print(response2)"]},{"cell_type":"code","execution_count":105,"metadata":{"executionInfo":{"elapsed":27,"status":"ok","timestamp":1670430503956,"user":{"displayName":"Alissa Huynh","userId":"05064198293916290951"},"user_tz":-240},"id":"csvknguTWZJt"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.10.6 64-bit ('shims')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"1875e3b4d47a39b140ad93b3e0c174c085ac6dc8de52e317e3cc5345dcbc697b"}}},"nbformat":4,"nbformat_minor":0}
