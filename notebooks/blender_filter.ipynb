{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip -q install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration\n",
    "import torch\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BlenderbotTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")\n",
    "model = BlenderbotForConditionalGeneration.from_pretrained(\"facebook/blenderbot-400M-distill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./filter/trigger_words.txt\") as file:\n",
    "    trigger_words = [line.rstrip() for line in file]\n",
    "    \n",
    "with open(\"./filter/bad_words.txt\") as file:\n",
    "    bad_words = [line.rstrip() for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_preprocessing(sentence):    \n",
    "    # lower all words\n",
    "    sentence = sentence.lower()    \n",
    "    # remove punctuation\n",
    "    for punctuation in string.punctuation:\n",
    "        sentence = sentence.replace(punctuation, '')        \n",
    "    # strip withespaces\n",
    "    sentence = sentence.strip()    \n",
    "    return sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_bad_words(sentence, bad_words): \n",
    "    \n",
    "    # preprocessing\n",
    "    sentence = filter_preprocessing(sentence)\n",
    "    \n",
    "    # Check\n",
    "    for word in sentence.split():        \n",
    "        if word in bad_words:\n",
    "            return True                \n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_trigger_words(sentence, trigger_words):\n",
    "    \n",
    "    # preprocessing\n",
    "    sentence = filter_preprocessing(sentence)\n",
    "    \n",
    "    # Check        \n",
    "    for word in sentence.split():        \n",
    "        if word in trigger_words:\n",
    "            return True                \n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input, history=[]):\n",
    "\n",
    "#preprocessing input  \n",
    "    filter_preprocessing(input)\n",
    "\n",
    "    if len(history) != 0:\n",
    "      input = ' '.join((*history, input))\n",
    "\n",
    "#check for potential triggers\n",
    "    if filter_trigger_words(input, trigger_words) is True:\n",
    "      response = \"A therapist will be in contact with you shortly.\"\n",
    "      return response, history\n",
    "  \n",
    "#check for potential bad words\n",
    "    elif filter_bad_words(input, bad_words) is True:\n",
    "      response = \"Let's try and say this a bit nicer.\"\n",
    "      return response, history\n",
    "  \n",
    "#if neither triggers nor bad words are present, generate a model output\n",
    "    else:\n",
    "      input_token = tokenizer(input, return_tensors='pt')\n",
    "    # print(inputs_token)\n",
    "\n",
    "      result = model.generate(**input_token)\n",
    "      outputs = tokenizer.decode(result[0])\n",
    "\n",
    "      history.append(' '.join((input, outputs)))\n",
    "\n",
    "      return outputs, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ann-SophieBlank/.pyenv/versions/MHConvoAI/lib/python3.10/site-packages/transformers/generation/utils.py:1387: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 60 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('<s> What do you need help with? I can help you if you want to talk about anything.</s>',\n",
       " ['Please help me. <s> What do you need help with? I can help you if you want to talk about anything.</s>'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input5 = 'Please help me.'\n",
    "history5 = []\n",
    "predict(input5, history5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('A therapist will be in contact with you shortly.', [])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input6 = 'Fuck everybody, I want to kill myself.'\n",
    "history6 = []\n",
    "predict(input6, history6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit ('shims')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1875e3b4d47a39b140ad93b3e0c174c085ac6dc8de52e317e3cc5345dcbc697b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
